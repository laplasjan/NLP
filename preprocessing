from langdetect import detect
def detect_lang(txt):
    try:
        return detect(txt)
    except:
        return np.nan
df.head(3)

df_head = ["index", "Narrative", "Fakes", "Disinfo_cases_en", "Date", "Countries","Media","Link","Debunking"]
df.head(0)

from nltk.tokenize import sent_tokenize, word_tokenize
import csv

reader = pd.read_csv(open('file1.csv'), delimiter= ";",quotechar = '"')
for line in reader:
    tokens = word_tokenize(file1.csv)
    
    import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
print(stopwords.words('english'))

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
import re


def vectorize(value, word_embeddings, dim = 632):
    sentences = value.to_list()

    sentence_vectors = []
    for i in sentences:
        if len(i) != 0:
            v = sum([word_embeddings.get(w, np.zeros((dim,))) for w in i.split()])/(len(i.split())+0.001)
        else:
            v = np.zeros((dim,))
        sentence_vectors.append(v)

    sentence_vectors = np.array(sentence_vectors)
    
    return sentence_vectors
    
    df_train[1:622]=df_train[1:622].apply(remove_contractions)
test[1:622]=test[1:622].apply(remove_contractions)

